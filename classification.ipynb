{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the MNIST data provided by Tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#Loading in the mnist data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images,\\\n",
    "    mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    #\"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set all variables\n",
    "\n",
    "# number of neurons in each layer\n",
    "input_num_units = 800\n",
    "hidden_num_units = 500\n",
    "output_num_units = 10\n",
    "\n",
    "# define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining variables\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "### define weights and biases of the neural network (refer this article if you don't understand the terminologies)\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer,labels= y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from DBN model\n",
    "train=load('rbm_dbn_troutput.npy')\n",
    "train_x=train\n",
    "test=load('rbm_dbn_teoutput.npy')\n",
    "test_x=test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1): \n",
    "    with tf.Session() as sess:\n",
    "        # create initialized variable\n",
    "        sess.run(init)\n",
    "    \n",
    "        ### for each epoch, do:\n",
    "        ###   for each batch, do:\n",
    "        ###     create pre-processed batch\n",
    "        ###     run optimizer by feeding batch\n",
    "        ###     find cost and reiterate to minimize\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            total_batch = int(train_x.shape[0]/batch_size)\n",
    "            for start, end in zip(range(0, len(train_x), batch_size), range(batch_size, len(train_x), batch_size)):\n",
    "                #batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "                batch_x = train_x[start:end]\n",
    "                y_val = trY[start:end]\n",
    "                batch_y = dense_to_one_hot(y_val)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "                avg_cost += c / total_batch\n",
    "            \n",
    "            print (\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "        print(\"\\nTraining complete!\")\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "   # pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "   # accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "   # print( \"Validation Accuracy:\", accuracy.eval({x: train_x.reshape(-1, input_num_units), y: dense_to_one_hot(trY)}))\n",
    "    \n",
    "        predict = tf.argmax(output_layer, 1)\n",
    "        pred = predict.eval({x: test_x.reshape(-1, input_num_units)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of DBN\n",
    "\n",
    "print(np.sum(pred==teY)/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from DAN model\n",
    "train=load('rbm_dan_troutput.npy')\n",
    "train_x=train\n",
    "test=load('rbm_dan_teoutput.npy')\n",
    "test_x=test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1): \n",
    "    with tf.Session() as sess:\n",
    "        # create initialized variable\n",
    "        sess.run(init)\n",
    "    \n",
    "        ### for each epoch, do:\n",
    "        ###   for each batch, do:\n",
    "        ###     create pre-processed batch\n",
    "        ###     run optimizer by feeding batch\n",
    "        ###     find cost and reiterate to minimize\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            avg_cost = 0\n",
    "            total_batch = int(train_x.shape[0]/batch_size)\n",
    "            for start, end in zip(range(0, len(train_x), batch_size), range(batch_size, len(train_x), batch_size)):\n",
    "                #batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "                batch_x = train_x[start:end]\n",
    "                y_val = trY[start:end]\n",
    "                batch_y = dense_to_one_hot(y_val)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "                avg_cost += c / total_batch\n",
    "            \n",
    "            print (\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "        print(\"\\nTraining complete!\")\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "   # pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "   # accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "   # print( \"Validation Accuracy:\", accuracy.eval({x: train_x.reshape(-1, input_num_units), y: dense_to_one_hot(trY)}))\n",
    "    \n",
    "        predict = tf.argmax(output_layer, 1)\n",
    "        pred = predict.eval({x: test_x.reshape(-1, input_num_units)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of DAN\n",
    "print(np.sum(pred==teY)/10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
